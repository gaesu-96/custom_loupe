{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "531bfd29",
   "metadata": {},
   "source": [
    "In this notebook, we transform raw datasets to parquet format to enable faster loading speed during training and evaluation.\n",
    "\n",
    "The raw format of released datasets is as follows:\n",
    "```python\n",
    "# train set\n",
    "/train/real/...\n",
    "/train/fake/...\n",
    "/train/masks/...\n",
    "# valid set\n",
    "/valid/real/...\n",
    "/valid/fake/...\n",
    "/valid/masks/...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bd7e9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import Dataset, DatasetDict\n",
    "from datasets import Features, Image, Value\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "def load_images_from_dir(directory: str) -> List[str]:\n",
    "    return [\n",
    "        os.path.join(directory, fname)\n",
    "        for fname in os.listdir(directory)\n",
    "        if fname.endswith((\"jpg\", \"jpeg\", \"png\", \"tif\"))\n",
    "    ]\n",
    "\n",
    "\n",
    "def create_split(root_dir: str, split: str) -> Optional[Dataset]:\n",
    "    fake_dir = os.path.join(root_dir, split, \"fake\")\n",
    "    real_dir = os.path.join(root_dir, split, \"real\")\n",
    "\n",
    "    if all(not os.path.isdir(p) for p in [fake_dir, real_dir]):\n",
    "        return None\n",
    "\n",
    "    print(f\"Split: {split},\", end=\" \")\n",
    "    fake_images, real_images = [], []\n",
    "    if os.path.isdir(fake_dir):\n",
    "        fake_images = load_images_from_dir(fake_dir)\n",
    "        print(f\"Fake images: {len(fake_images)}\", end=\"\")\n",
    "    if os.path.isdir(real_dir):\n",
    "        real_images = load_images_from_dir(real_dir)\n",
    "        print(f\", Real images: {len(real_images)}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "    return Dataset.from_dict(\n",
    "        {\n",
    "            \"path\": fake_images + real_images,\n",
    "            \"image\": fake_images + real_images,\n",
    "        },\n",
    "        features=Features(\n",
    "            {\"path\": Value(dtype=\"string\"), \"image\": Image()}\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def create_dataset(root_dir: str) -> DatasetDict:\n",
    "    return DatasetDict(\n",
    "        {\n",
    "            split: d\n",
    "            for split in [\"train\", \"valid\", \"test\"]\n",
    "            if (d := create_split(root_dir, split)) is not None\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "# replace with your own dataset path\n",
    "root_dir = \"../dataset/\"\n",
    "save_dir = \"./data_output\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d6f1c7",
   "metadata": {},
   "source": [
    "We merge `real/` and `fake/` into `images` column for simplity. A image is real if there is no corresponding mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07009f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: train, Fake images: 16880, Real images: 19284\n",
      "Split: valid, Fake images: 2110, Real images: 2411\n",
      "Split: test, Fake images: 2110, Real images: 2411\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['path', 'image'],\n",
       "        num_rows: 36164\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['path', 'image'],\n",
       "        num_rows: 4521\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['path', 'image'],\n",
       "        num_rows: 4521\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = create_dataset(root_dir)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa7de84",
   "metadata": {},
   "source": [
    "Then save processed datasets to parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd6b20bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|█████████████| 140/140 [00:00<00:00, 5950.33ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train split to ./data_output/train.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|███████████████| 18/18 [00:00<00:00, 5703.09ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved valid split to ./data_output/valid.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|███████████████| 18/18 [00:00<00:00, 1376.96ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved test split to ./data_output/test.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(save_dir, exist_ok=True)\n",
    "for split in dataset:\n",
    "    dataset[split].to_parquet(os.path.join(save_dir, f\"{split}.parquet\"))\n",
    "    print(f\"Saved {split} split to {save_dir}/{split}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63933c8",
   "metadata": {},
   "source": [
    "Load from processed datasets to do whatever you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4af7f346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 36164 examples [00:00, 920087.17 examples/s]\n",
      "Generating validation split: 4521 examples [00:00, 1195313.19 examples/s]\n",
      "Generating test split: 4521 examples [00:00, 1254130.18 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['path', 'image'],\n",
       "    num_rows: 36164\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "trainset = load_dataset(\"parquet\", data_dir=save_dir, split=\"train\")\n",
    "trainset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c84f0a",
   "metadata": {},
   "source": [
    "Since the forged components are usually smaller in proportion compared to the real ones, this leads to class imbalance.\n",
    "For optimal training performance, hyper parameters such as `pixel_forge_weight` and `cls_forge_weight` in `src.loupe.configuration_loupe.LoupeConfig` must be appropriately configured. These parameters control the weights of forged pixels and forged images.\n",
    "\n",
    "Once suitable parameters are found using the following code snippet, you can set them in `configs/model/cls.yaml` or `configs/model/seg.yaml`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40a5ec91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing mask stats (num_proc=8):   0%|                  | 0/5000 [00:00<?, ? examples/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRemoteTraceback\u001b[39m                           Traceback (most recent call last)",
      "\u001b[31mRemoteTraceback\u001b[39m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/gaesu96/miniconda3/envs/loupe/lib/python3.11/site-packages/multiprocess/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/gaesu96/miniconda3/envs/loupe/lib/python3.11/site-packages/datasets/utils/py_utils.py\", line 586, in _write_generator_to_queue\n    for i, result in enumerate(func(**kwargs)):\n  File \"/Users/gaesu96/miniconda3/envs/loupe/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 3664, in _map_single\n    for i, example in iter_outputs(shard_iterable):\n  File \"/Users/gaesu96/miniconda3/envs/loupe/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 3638, in iter_outputs\n    yield i, apply_function(example, i, offset=offset)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/gaesu96/miniconda3/envs/loupe/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 3561, in apply_function\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/folders/y4/81g32gm97639d4rm8_3c076r0000gn/T/ipykernel_32950/967571456.py\", line 18, in compute_mask_stats\n    if example[\"mask\"] is None:\n       ~~~~~~~^^^^^^^^\n  File \"/Users/gaesu96/miniconda3/envs/loupe/lib/python3.11/site-packages/datasets/formatting/formatting.py\", line 283, in __getitem__\n    value = self.data[key]\n            ~~~~~~~~~^^^^^\nKeyError: 'mask'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     42\u001b[39m     forged_patch_sum = forged_patch_sum.sum()\n\u001b[32m     44\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     45\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mis_forge\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1\u001b[39m,\n\u001b[32m     46\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mforge_pixel_sum\u001b[39m\u001b[33m\"\u001b[39m: forged_pixel_sum,\n\u001b[32m     47\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtotal_pixel_count\u001b[39m\u001b[33m\"\u001b[39m: total_pixels,\n\u001b[32m     48\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mforge_patch_sum\u001b[39m\u001b[33m\"\u001b[39m: forged_patch_sum,\n\u001b[32m     49\u001b[39m     }\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m processed = \u001b[43msubset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_mask_stats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mComputing mask stats\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m num_forge_images = \u001b[38;5;28msum\u001b[39m(processed[\u001b[33m\"\u001b[39m\u001b[33mis_forge\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     55\u001b[39m num_forge_pixels = \u001b[38;5;28msum\u001b[39m(processed[\u001b[33m\"\u001b[39m\u001b[33mforge_pixel_sum\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/loupe/lib/python3.11/site-packages/datasets/arrow_dataset.py:562\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    555\u001b[39m self_format = {\n\u001b[32m    556\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    557\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    558\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    559\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    560\u001b[39m }\n\u001b[32m    561\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m562\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    563\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/loupe/lib/python3.11/site-packages/datasets/arrow_dataset.py:3323\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[39m\n\u001b[32m   3320\u001b[39m os.environ = prev_env\n\u001b[32m   3321\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSpawning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m processes\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m3323\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miflatmap_unordered\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3324\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_single\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs_iterable\u001b[49m\u001b[43m=\u001b[49m\u001b[43munprocessed_kwargs_per_job\u001b[49m\n\u001b[32m   3325\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3326\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheck_if_shard_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3328\u001b[39m pool.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/loupe/lib/python3.11/site-packages/datasets/utils/py_utils.py:626\u001b[39m, in \u001b[36miflatmap_unordered\u001b[39m\u001b[34m(pool, func, kwargs_iterable)\u001b[39m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    624\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[32m    625\u001b[39m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m         \u001b[43m[\u001b[49m\u001b[43masync_result\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43masync_result\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43masync_results\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/loupe/lib/python3.11/site-packages/datasets/utils/py_utils.py:626\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    624\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[32m    625\u001b[39m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m         [\u001b[43masync_result\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/loupe/lib/python3.11/site-packages/multiprocess/pool.py:774\u001b[39m, in \u001b[36mApplyResult.get\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    772\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value\n",
      "\u001b[31mKeyError\u001b[39m: 'mask'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "cls_forge_weight: float  # the ratio of forged images to total images.\n",
    "# the ratio of forged patches to total patches across all images.\n",
    "patch_forge_weight: float\n",
    "# the ratio of forged pixels to total pixels across fake images.\n",
    "pixel_forge_weight: float\n",
    "\n",
    "num_subset_samples = min(5000, len(trainset))\n",
    "subset = trainset.shuffle().select(range(num_subset_samples))\n",
    "image_size, patch_size = 336, 14\n",
    "\n",
    "\n",
    "def compute_mask_stats(example):\n",
    "\n",
    "    if example[\"mask\"] is None:\n",
    "        return {\n",
    "            \"is_forge\": 0,\n",
    "            \"forge_pixel_sum\": 0.0,\n",
    "            \"total_pixel_count\": 0,\n",
    "            \"forge_patch_sum\": 0.0,\n",
    "        }\n",
    "\n",
    "    mask = example[\"mask\"].convert(\"L\").resize((image_size, image_size), Image.NEAREST)\n",
    "    mask_np = np.array(mask, dtype=np.float32)\n",
    "\n",
    "    if mask_np.max() != mask_np.min():\n",
    "        mask_np = (mask_np - mask_np.min()) / (mask_np.max() - mask_np.min())\n",
    "    else:\n",
    "        mask_np[:] = 0.0\n",
    "\n",
    "    forged_pixel_sum = mask_np.sum()\n",
    "    total_pixels = mask_np.size\n",
    "\n",
    "    reshaped = mask_np.reshape(\n",
    "        image_size // patch_size, patch_size, image_size // patch_size, patch_size\n",
    "    )\n",
    "    patches = reshaped.transpose(0, 2, 1, 3)\n",
    "    forged_patch_sum = (patches != 0).sum(axis=(2, 3)) / (patch_size * patch_size)\n",
    "    forged_patch_sum = forged_patch_sum.sum()\n",
    "\n",
    "    return {\n",
    "        \"is_forge\": 1,\n",
    "        \"forge_pixel_sum\": forged_pixel_sum,\n",
    "        \"total_pixel_count\": total_pixels,\n",
    "        \"forge_patch_sum\": forged_patch_sum,\n",
    "    }\n",
    "\n",
    "\n",
    "processed = subset.map(compute_mask_stats, num_proc=8, desc=\"Computing mask stats\")\n",
    "\n",
    "num_forge_images = sum(processed[\"is_forge\"])\n",
    "num_forge_pixels = sum(processed[\"forge_pixel_sum\"])\n",
    "num_total_pixels = sum(processed[\"total_pixel_count\"])\n",
    "num_forge_patches = sum(processed[\"forge_patch_sum\"])\n",
    "num_total_patches = len(processed) * (image_size // patch_size) ** 2\n",
    "\n",
    "cls_forge_weight = 1 - num_forge_images / len(processed)\n",
    "patch_forge_weight = 1 - num_forge_patches / num_total_patches\n",
    "pixel_forge_weight = 1 - num_forge_pixels / num_total_pixels\n",
    "\n",
    "print(\"cls_forge_weight:\", cls_forge_weight)\n",
    "print(\"patch_forge_weight:\", patch_forge_weight)\n",
    "print(\"pixel_forge_weight:\", pixel_forge_weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
